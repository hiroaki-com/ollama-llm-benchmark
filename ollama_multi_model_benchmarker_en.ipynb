{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiroaki-com/ollama-llm-benchmark/blob/main/ollama_multi_model_benchmarker_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtoltT_-8pnA"
      },
      "source": [
        "A completely free benchmarking tool for automatically comparing Ollama model performance in Google Colab environment.\n",
        "\n",
        "Key Features:\n",
        "- üîÑ Flexible Model Selection (Easy selection via comma-separated input + checkboxes)\n",
        "- üéØ Single Source of Truth Design (Model list managed in one place for easy editing)\n",
        "- üìä Comprehensive Performance Metrics (Measures generation speed, TTFT, model size, quantization level, etc.)\n",
        "- üíæ Automatic Result Saving (Saves integrated JSON, session archives, and size cache to `Google Drive` under `MyDrive`)\n",
        "- üìà Visualization Reports (Displays results instantly with graphs + Markdown tables + response previews)\n",
        "\n",
        "How to Use:\n",
        "1. üìã Run the `Model Registry` cell to load the model list\n",
        "2. ‚úÖ Select test targets using `checkboxes`\n",
        "3. üß™ Run the `Ollama Multi-Model Benchmarker` cell to start measurement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uP0AeHIo8pnC"
      },
      "outputs": [],
      "source": [
        "#@title üìã Model Registry\n",
        "\n",
        "# @markdown Model Configuration\n",
        "# @markdown - Enter test target models separated by commas. Search for model names at https://ollama.com/search and verify the official names.\n",
        "# @markdown\n",
        "# @markdown - üí° Selection guideline for `T4 GPU` environment: `8B` (recommended) | `14B` (practical) | `20B`+ (slow)\n",
        "\n",
        "model_list = \"qwen3:8b, qwen3:14b, qwen2.5-coder:7b, qwen2.5-coder:14b, ministral-3:8b, ministral-3:14b, devstral-small-2:24b, gpt-oss:20b, deepseek-r1:8b\" #@param {type:\"string\"}\n",
        "\n",
        "AVAILABLE_MODELS = [\n",
        "    model.strip()\n",
        "    for model in model_list.split(',')\n",
        "    if model.strip()\n",
        "]\n",
        "\n",
        "if not AVAILABLE_MODELS:\n",
        "    raise ValueError(\"‚ùå Model list is empty. Please enter at least one model.\")\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def create_model_selector():\n",
        "    checkboxes = []\n",
        "\n",
        "    select_all = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='‚úÖ Select All Models',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='300px')\n",
        "    )\n",
        "\n",
        "    model_checks = {}\n",
        "    for model in AVAILABLE_MODELS:\n",
        "        cb = widgets.Checkbox(\n",
        "            value=True,\n",
        "            description=model,\n",
        "            indent=False,\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=widgets.Layout(width='300px')\n",
        "        )\n",
        "        model_checks[model] = cb\n",
        "        checkboxes.append(cb)\n",
        "\n",
        "    def on_select_all_change(change):\n",
        "        for cb in checkboxes:\n",
        "            cb.value = change['new']\n",
        "\n",
        "    select_all.observe(on_select_all_change, names='value')\n",
        "\n",
        "    header = widgets.HTML('<h3>üì¶ Available Models</h3><p style=\"margin: 5px 0 10px 0; font-size: 13px;\">Select all or uncheck individually to narrow down test targets. After selection, run the next cell.</p>')\n",
        "    separator = widgets.HTML('<hr style=\"margin: 10px 0;\">')\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        header,\n",
        "        select_all,\n",
        "        separator,\n",
        "        widgets.VBox(checkboxes, layout=widgets.Layout(padding='0 0 0 20px'))\n",
        "    ])\n",
        "\n",
        "    display(ui)\n",
        "\n",
        "    return select_all, model_checks\n",
        "\n",
        "select_all_widget, model_checkboxes = create_model_selector()\n",
        "\n",
        "print(f\"‚úÖ Model list loaded: {len(AVAILABLE_MODELS)} models available.\")\n",
        "print(\"‚û°Ô∏è Please run the next cell (Benchmarker).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QwBlqGb08pnD"
      },
      "outputs": [],
      "source": [
        "#@title üß™ Ollama Multi-Model Benchmarker\n",
        "\n",
        "# @markdown Benchmark Configuration\n",
        "# @markdown - `save_to_drive`: Set to `True` to save results to Google Drive\n",
        "# @markdown - `timeout_seconds`: Maximum processing time per model (seconds)\n",
        "# @markdown - `custom_test_prompt`: If left blank, default prompt (Python code generation) will be used\n",
        "# @markdown\n",
        "\n",
        "save_to_drive = False #@param {type:\"boolean\"}\n",
        "timeout_seconds = 1000 #@param {type:\"integer\"}\n",
        "custom_test_prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import shutil\n",
        "import warnings\n",
        "import atexit\n",
        "import psutil\n",
        "import platform\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    OLLAMA_BINARY: str = \"/usr/local/bin/ollama\"\n",
        "    OLLAMA_HOST: str = \"0.0.0.0:11434\"\n",
        "    OLLAMA_API_BASE: str = \"http://0.0.0.0:11434\"\n",
        "\n",
        "    PULL_MAX_RETRIES: int = 3\n",
        "    PULL_BACKOFF_BASE: int = 5\n",
        "\n",
        "    SERVER_STARTUP_MAX_ATTEMPTS: int = 30\n",
        "    SERVER_STARTUP_POLL_INTERVAL: int = 1\n",
        "    SERVER_HEALTH_CHECK_TIMEOUT: int = 2\n",
        "\n",
        "    WARMUP_TIMEOUT: int = 300\n",
        "    WARMUP_NUM_PREDICT: int = 1\n",
        "    MODEL_UNLOAD_WAIT: int = 2\n",
        "    MODEL_UNLOAD_TIMEOUT: int = 10\n",
        "\n",
        "    DEFAULT_PROMPT: str = \"Write a recursive Python function with type hints and a docstring to compute the factorial of a number, test it with n = 5, and show only the code and the expected result.\"\n",
        "    MAX_PROMPT_CHARS: int = 500\n",
        "    MAX_RESPONSE_DISPLAY_CHARS: int = 1500\n",
        "\n",
        "    DISK_SAFETY_MARGIN_GB: int = 2\n",
        "    UNKNOWN_MODEL_MIN_FREE_GB: int = 20\n",
        "\n",
        "    NUM_CTX: int = 4096\n",
        "    TEMPERATURE: float = 0.0\n",
        "\n",
        "    CACHE_FILENAME: str = \"model_size_cache.json\"\n",
        "\n",
        "class C:\n",
        "    RESET = '\\033[0m'\n",
        "    GREEN = '\\033[32m'\n",
        "    RED = '\\033[31m'\n",
        "    YELLOW = '\\033[33m'\n",
        "    BLUE = '\\033[34m'\n",
        "    CYAN = '\\033[36m'\n",
        "    MAGENTA = '\\033[35m'\n",
        "    WHITE = '\\033[37m'\n",
        "    DIM = '\\033[2m'\n",
        "    BOLD = '\\033[1m'\n",
        "\n",
        "config = Config()\n",
        "session_id = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "ollama_process: Optional[subprocess.Popen] = None\n",
        "\n",
        "try:\n",
        "    if select_all_widget.value:\n",
        "        selected_models = AVAILABLE_MODELS.copy()\n",
        "    else:\n",
        "        selected_models = [\n",
        "            model for model, checkbox in model_checkboxes.items()\n",
        "            if checkbox.value\n",
        "        ]\n",
        "except NameError:\n",
        "    print(f\"{C.RED}‚ùå Error: Model Registry not loaded{C.RESET}\")\n",
        "    print(f\"{C.YELLOW}Please run the 'Model Registry' cell first.{C.RESET}\")\n",
        "    raise SystemExit(\"Model Registry cell must be executed before benchmark\")\n",
        "\n",
        "if not selected_models:\n",
        "    print(f\"{C.RED}‚ùå Error: No models selected{C.RESET}\")\n",
        "    print(f\"{C.YELLOW}Please select at least one model in the Model Registry cell.{C.RESET}\")\n",
        "    raise SystemExit(\"At least one model must be selected\")\n",
        "\n",
        "def cleanup_ollama_server() -> None:\n",
        "    global ollama_process\n",
        "    if ollama_process:\n",
        "        try:\n",
        "            ollama_process.terminate()\n",
        "            ollama_process.wait(timeout=5)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            try:\n",
        "                ollama_process.kill()\n",
        "            except ProcessLookupError:\n",
        "                pass\n",
        "        except ProcessLookupError:\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(f\"{C.YELLOW}Warning: Failed to cleanup Ollama process: {e}{C.RESET}\", file=sys.stderr)\n",
        "\n",
        "def trim_to_boundary(text: str, limit: int) -> str:\n",
        "    if len(text) <= limit:\n",
        "        return text\n",
        "    candidate = text[:limit]\n",
        "    for sep in [\"\\n\", \"„ÄÇ\", \".\", \"„ÄÅ\", \",\", \" \"]:\n",
        "        idx = candidate.rfind(sep)\n",
        "        if idx > limit // 2:\n",
        "            return candidate[:idx + len(sep)].rstrip()\n",
        "    return candidate.rstrip()\n",
        "\n",
        "if custom_test_prompt.strip():\n",
        "    resolved_prompt = trim_to_boundary(custom_test_prompt.strip(), config.MAX_PROMPT_CHARS)\n",
        "else:\n",
        "    resolved_prompt = config.DEFAULT_PROMPT\n",
        "\n",
        "TEST_PROMPTS: List[Dict[str, str]] = [\n",
        "    {\n",
        "        \"name\": \"Python Factorial\" if not custom_test_prompt.strip() else \"Custom Prompt\",\n",
        "        \"prompt\": resolved_prompt,\n",
        "        \"expected\": \"Recursive factorial function\" if not custom_test_prompt.strip() else \"Custom\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Ollama Multi-Model Benchmarker\")\n",
        "print(f\"Models: {len(selected_models)} | Timeout: {timeout_seconds}s\")\n",
        "if custom_test_prompt.strip() and len(custom_test_prompt.strip()) != len(resolved_prompt):\n",
        "    print(f\"{C.YELLOW}  ‚Ä∫ Prompt truncated at boundary ({len(resolved_prompt)} chars){C.RESET}\")\n",
        "print()\n",
        "print(\"Selected Models:\")\n",
        "for idx, model in enumerate(selected_models, 1):\n",
        "    print(f\"  {idx}. {model}\")\n",
        "print()\n",
        "print(\"Metrics Definition:\")\n",
        "print(f\"  {'t/s':<10} : Tokens per Second ... Generation speed\")\n",
        "print(f\"  {'TTFT':<10} : Time To First Token . Response latency\")\n",
        "print(f\"  {'Total':<10} : End-to-End Time ..... Total processing time\")\n",
        "print(f\"  {'Size':<10} : Model Size .......... Disk/VRAM usage\")\n",
        "print()\n",
        "\n",
        "if save_to_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    BASE_DIR = '/content/drive/MyDrive/OllamaBenchmarks'\n",
        "    RESULTS_FILE = f'{BASE_DIR}/benchmark_results.json'\n",
        "    ARCHIVE_DIR = f'{BASE_DIR}/session_logs'\n",
        "    CACHE_FILE = f'{BASE_DIR}/{config.CACHE_FILENAME}'\n",
        "\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n",
        "else:\n",
        "    BASE_DIR = None\n",
        "    RESULTS_FILE = None\n",
        "    ARCHIVE_DIR = None\n",
        "    CACHE_FILE = None\n",
        "\n",
        "def load_size_cache() -> Dict[str, float]:\n",
        "    if save_to_drive and CACHE_FILE and os.path.exists(CACHE_FILE):\n",
        "        try:\n",
        "            with open(CACHE_FILE, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"{C.YELLOW}Warning: Failed to load size cache: {e}{C.RESET}\")\n",
        "    return {}\n",
        "\n",
        "def update_size_cache(model_name: str, size_gb: float) -> None:\n",
        "    if not save_to_drive or not CACHE_FILE:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        cache = load_size_cache()\n",
        "        cache[model_name] = size_gb\n",
        "        with open(CACHE_FILE, 'w') as f:\n",
        "            json.dump(cache, f, indent=2)\n",
        "    except Exception as e:\n",
        "        print(f\"{C.YELLOW}Warning: Failed to update size cache: {e}{C.RESET}\")\n",
        "\n",
        "def get_disk_usage() -> Dict[str, float]:\n",
        "    total, used, free = shutil.disk_usage(\"/\")\n",
        "    return {\n",
        "        \"total_gb\": round(total / (1024**3), 2),\n",
        "        \"used_gb\": round(used / (1024**3), 2),\n",
        "        \"free_gb\": round(free / (1024**3), 2)\n",
        "    }\n",
        "\n",
        "def get_cpu_info() -> str:\n",
        "    try:\n",
        "        if platform.system() == \"Linux\":\n",
        "            with open(\"/proc/cpuinfo\", \"r\") as f:\n",
        "                for line in f:\n",
        "                    if \"model name\" in line:\n",
        "                        return line.split(\":\")[1].strip()\n",
        "        return platform.processor() or \"Unknown CPU\"\n",
        "    except:\n",
        "        return \"Unknown CPU\"\n",
        "\n",
        "def get_system_info() -> Dict[str, Any]:\n",
        "    try:\n",
        "        gpu_info = !nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits\n",
        "        if gpu_info:\n",
        "            parts = gpu_info[0].split(',')\n",
        "            gpu_name = parts[0].strip()\n",
        "            vram_gb = int(parts[1].strip())\n",
        "        else:\n",
        "            gpu_name = \"Unknown/None\"\n",
        "            vram_gb = 0\n",
        "    except Exception as e:\n",
        "        gpu_name = \"Unknown\"\n",
        "        vram_gb = 0\n",
        "\n",
        "    cpu_name = get_cpu_info()\n",
        "    cpu_cores = os.cpu_count()\n",
        "    try:\n",
        "        ram_obj = psutil.virtual_memory()\n",
        "        ram_total_gb = round(ram_obj.total / (1024**3), 2)\n",
        "        ram_available_gb = round(ram_obj.available / (1024**3), 2)\n",
        "    except Exception:\n",
        "        ram_total_gb = 0\n",
        "        ram_available_gb = 0\n",
        "\n",
        "    disk_info = get_disk_usage()\n",
        "\n",
        "    return {\n",
        "        \"gpu\": gpu_name,\n",
        "        \"vram_gb\": vram_gb,\n",
        "        \"cpu\": cpu_name,\n",
        "        \"cpu_cores\": cpu_cores,\n",
        "        \"ram_total_gb\": ram_total_gb,\n",
        "        \"ram_available_gb\": ram_available_gb,\n",
        "        \"disk_total_gb\": disk_info[\"total_gb\"],\n",
        "        \"disk_free_gb\": disk_info[\"free_gb\"],\n",
        "        \"platform\": platform.platform()\n",
        "    }\n",
        "\n",
        "def get_model_details(model_name: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/show\",\n",
        "            json={\"name\": model_name},\n",
        "            timeout=5\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            generate_response_data = response.json()\n",
        "            details = generate_response_data.get(\"details\", {})\n",
        "            return {\n",
        "                \"quantization\": details.get(\"quantization_level\", \"Unknown\"),\n",
        "                \"family\": details.get(\"family\", \"Unknown\"),\n",
        "                \"parameter_size\": details.get(\"parameter_size\", \"Unknown\")\n",
        "            }\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {\"quantization\": \"Unknown\", \"family\": \"Unknown\", \"parameter_size\": \"Unknown\"}\n",
        "\n",
        "def get_installed_model_size(model_name: str) -> Optional[float]:\n",
        "    try:\n",
        "        response = requests.get(f\"{config.OLLAMA_API_BASE}/api/tags\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            models = response.json().get(\"models\", [])\n",
        "            for m in models:\n",
        "                if m[\"name\"] == model_name or m[\"name\"] == f\"{model_name}:latest\":\n",
        "                    size_bytes = m.get(\"size\", 0)\n",
        "                    return round(size_bytes / (1024**3), 2)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq zstd\n",
        "!pip install -q psutil matplotlib ipywidgets\n",
        "\n",
        "print()\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print()\n",
        "\n",
        "os.environ['OLLAMA_HOST'] = config.OLLAMA_HOST\n",
        "os.environ['OLLAMA_KEEP_ALIVE'] = '5m'\n",
        "os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'\n",
        "os.environ['OLLAMA_FLASH_ATTENTION'] = '1'\n",
        "\n",
        "startup_start = time.time()\n",
        "\n",
        "ollama_process = subprocess.Popen(\n",
        "    [config.OLLAMA_BINARY, \"serve\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "for attempt in range(config.SERVER_STARTUP_MAX_ATTEMPTS):\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/tags\",\n",
        "            timeout=config.SERVER_HEALTH_CHECK_TIMEOUT\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            startup_time = round(time.time() - startup_start, 2)\n",
        "            print(f\"{C.GREEN}‚úÖ Ollama server ready in {startup_time}s{C.RESET}\")\n",
        "            atexit.register(cleanup_ollama_server)\n",
        "            break\n",
        "    except requests.RequestException:\n",
        "        pass\n",
        "    time.sleep(config.SERVER_STARTUP_POLL_INTERVAL)\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå Failed to start server\")\n",
        "\n",
        "sys_info = get_system_info()\n",
        "print()\n",
        "print(f\"{C.BOLD}System Information ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ{C.RESET}\")\n",
        "print(f\"  OS   : {sys_info['platform']}\")\n",
        "print(f\"  CPU  : {sys_info['cpu']} ({sys_info['cpu_cores']} cores)\")\n",
        "print(f\"  RAM  : {sys_info['ram_total_gb']} GB\")\n",
        "print(f\"  GPU  : {sys_info['gpu']} ({sys_info['vram_gb']} GB VRAM)\")\n",
        "print()\n",
        "\n",
        "def pull_model_with_retry(model_name: str, env: Dict[str, str], timeout: int) -> subprocess.CompletedProcess:\n",
        "    for attempt in range(1, config.PULL_MAX_RETRIES + 1):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [config.OLLAMA_BINARY, \"pull\", model_name],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=timeout,\n",
        "                env=env\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                return result\n",
        "            if attempt < config.PULL_MAX_RETRIES:\n",
        "                wait = config.PULL_BACKOFF_BASE * (2 ** (attempt - 1))\n",
        "                print(f\"  {C.YELLOW}‚Ä∫ {'Retry':<7}{C.RESET} Pull failed (attempt {attempt}/{config.PULL_MAX_RETRIES}) ‚Äî retrying in {wait}s ...\")\n",
        "                time.sleep(wait)\n",
        "        except subprocess.TimeoutExpired as e:\n",
        "            if attempt >= config.PULL_MAX_RETRIES:\n",
        "                raise\n",
        "            wait = config.PULL_BACKOFF_BASE * (2 ** (attempt - 1))\n",
        "            print(f\"  {C.YELLOW}‚Ä∫ {'Retry':<7}{C.RESET} Pull timeout (attempt {attempt}/{config.PULL_MAX_RETRIES}) ‚Äî retrying in {wait}s ...\")\n",
        "            time.sleep(wait)\n",
        "    raise RuntimeError(f\"Pull failed after {config.PULL_MAX_RETRIES} attempts: {result.stderr}\")\n",
        "\n",
        "def warmup_model(model_name: str) -> float:\n",
        "    try:\n",
        "        warmup_res = requests.post(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/generate\",\n",
        "            json={\n",
        "                \"model\": model_name,\n",
        "                \"prompt\": \"warmup\",\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"num_ctx\": config.NUM_CTX,\n",
        "                    \"num_predict\": config.WARMUP_NUM_PREDICT\n",
        "                }\n",
        "            },\n",
        "            timeout=config.WARMUP_TIMEOUT\n",
        "        )\n",
        "        if warmup_res.status_code == 200:\n",
        "            warmup_data = warmup_res.json()\n",
        "            return round(warmup_data.get(\"total_duration\", 0) / 1e9, 2)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"  {C.YELLOW}‚Ä∫ {'Warning':<7}{C.RESET} Warmup failed: {e}\", file=sys.stderr)\n",
        "    return 0.0\n",
        "\n",
        "def unload_model(model_name: str) -> None:\n",
        "    try:\n",
        "        requests.post(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/generate\",\n",
        "            json={\"model\": model_name, \"keep_alive\": 0},\n",
        "            timeout=config.MODEL_UNLOAD_TIMEOUT\n",
        "        )\n",
        "        time.sleep(config.MODEL_UNLOAD_WAIT)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"  {C.YELLOW}‚Ä∫ {'Warning':<7}{C.RESET} Unload failed: {e}\", file=sys.stderr)\n",
        "\n",
        "def delete_model(model_name: str, env: Dict[str, str]) -> None:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [config.OLLAMA_BINARY, \"rm\", model_name],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            env=env,\n",
        "            timeout=30\n",
        "        )\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        print(f\"  {C.YELLOW}‚Ä∫ {'Warning':<7}{C.RESET} Delete timeout: {e}\", file=sys.stderr)\n",
        "\n",
        "def save_benchmark_result(benchmark_result: Dict[str, Any], results_file: str) -> None:\n",
        "    try:\n",
        "        if os.path.exists(results_file):\n",
        "            with open(results_file, 'r', encoding='utf-8') as f:\n",
        "                all_data = json.load(f)\n",
        "        else:\n",
        "            all_data = {\n",
        "                \"schema_version\": \"1.1\",\n",
        "                \"last_updated\": None,\n",
        "                \"benchmarks\": []\n",
        "            }\n",
        "\n",
        "        all_data[\"benchmarks\"].append(benchmark_result)\n",
        "        all_data[\"last_updated\"] = datetime.utcnow().isoformat() + \"Z\"\n",
        "\n",
        "        temp_file = results_file + \".tmp\"\n",
        "        with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "        os.replace(temp_file, results_file)\n",
        "\n",
        "    except (IOError, json.JSONDecodeError) as e:\n",
        "        print(f\"  {C.YELLOW}‚Ä∫ {'Warning':<7}{C.RESET} Save error: {str(e)}\")\n",
        "\n",
        "session_start_time = datetime.utcnow()\n",
        "benchmark_results: List[Dict[str, Any]] = []\n",
        "successful_tests = 0\n",
        "failed_tests = 0\n",
        "\n",
        "model_size_cache = load_size_cache()\n",
        "\n",
        "selected_models.sort(key=lambda m: model_size_cache.get(m, 0))\n",
        "\n",
        "for model_idx, model_name in enumerate(selected_models, 1):\n",
        "    print(f\"{C.BLUE}‚ñ∂{C.RESET} {C.BOLD}[{model_idx}/{len(selected_models)}] {model_name}{C.RESET}\")\n",
        "\n",
        "    disk_before = get_disk_usage()\n",
        "\n",
        "    cached_size = model_size_cache.get(model_name)\n",
        "    if cached_size is not None:\n",
        "        required_space = cached_size + config.DISK_SAFETY_MARGIN_GB\n",
        "        size_str = f\"{cached_size}GB (cached)\"\n",
        "    else:\n",
        "        required_space = config.UNKNOWN_MODEL_MIN_FREE_GB\n",
        "        size_str = \"Unknown (Checking >20GB)\"\n",
        "\n",
        "    if disk_before['free_gb'] < required_space:\n",
        "        print(f\"  {C.RED}‚Ä∫ {'Skip':<7}{C.RESET} Insufficient disk space (need {required_space:.1f}GB, free {disk_before['free_gb']}GB)\")\n",
        "\n",
        "        failed_metrics = {\n",
        "            \"model\": model_name,\n",
        "            \"error\": \"Insufficient disk space\",\n",
        "            \"required_gb\": required_space,\n",
        "            \"free_gb\": disk_before['free_gb']\n",
        "        }\n",
        "        benchmark_results.append({\n",
        "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"model\": model_name,\n",
        "            \"environment\": sys_info,\n",
        "            \"metrics\": failed_metrics\n",
        "        })\n",
        "        failed_tests += 1\n",
        "        print()\n",
        "        continue\n",
        "\n",
        "    metrics: Dict[str, Any] = {\n",
        "        \"model\": model_name,\n",
        "        \"pull_time\": 0,\n",
        "        \"model_load_time\": 0,\n",
        "        \"model_size_gb\": cached_size if cached_size else 0,\n",
        "        \"meta\": {},\n",
        "        \"tests\": [],\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"  {C.DIM}‚Ä∫ {'Setup':<7}{C.RESET} Free: {disk_before['free_gb']}GB | Est. Size: {size_str}\")\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        env['OLLAMA_HOST'] = config.OLLAMA_HOST\n",
        "        env['HOME'] = '/root'\n",
        "\n",
        "        pull_start = time.time()\n",
        "        pull_model_with_retry(model_name, env, timeout_seconds)\n",
        "        metrics[\"pull_time\"] = round(time.time() - pull_start, 2)\n",
        "\n",
        "        model_details = get_model_details(model_name)\n",
        "        metrics[\"meta\"] = model_details\n",
        "        quant_disp = model_details['quantization']\n",
        "\n",
        "        real_size = get_installed_model_size(model_name)\n",
        "        if real_size:\n",
        "            metrics[\"model_size_gb\"] = real_size\n",
        "            if cached_size != real_size:\n",
        "                update_size_cache(model_name, real_size)\n",
        "                model_size_cache[model_name] = real_size\n",
        "            size_display = f\"{real_size}GB\"\n",
        "        else:\n",
        "            size_display = \"Unknown\"\n",
        "\n",
        "        print(f\"  {C.DIM}‚Ä∫ {'Pull':<7}{C.RESET} {C.GREEN}Download complete {metrics['pull_time']}s{C.RESET} {C.DIM}({size_display}, {quant_disp}){C.RESET}\")\n",
        "\n",
        "        metrics[\"model_load_time\"] = warmup_model(model_name)\n",
        "        print(f\"  {C.DIM}‚Ä∫ {'Load':<7}{C.RESET} {C.GREEN}VRAM load complete {metrics['model_load_time']}s{C.RESET}\")\n",
        "\n",
        "        for test_idx, test in enumerate(TEST_PROMPTS, 1):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    f\"{config.OLLAMA_API_BASE}/api/generate\",\n",
        "                    json={\n",
        "                        \"model\": model_name,\n",
        "                        \"prompt\": test[\"prompt\"],\n",
        "                        \"stream\": False,\n",
        "                        \"options\": {\n",
        "                            \"num_ctx\": config.NUM_CTX,\n",
        "                            \"temperature\": config.TEMPERATURE\n",
        "                        }\n",
        "                    },\n",
        "                    timeout=timeout_seconds\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    generate_response_data = response.json()\n",
        "                    response_text = generate_response_data.get(\"response\", \"\")\n",
        "\n",
        "                    test_metrics = {\n",
        "                        \"name\": test[\"name\"],\n",
        "                        \"prompt\": test[\"prompt\"],\n",
        "                        \"response\": response_text,\n",
        "                        \"total_time\": round(generate_response_data.get(\"total_duration\", 0) / 1e9, 2),\n",
        "                        \"first_token_time\": round(generate_response_data.get(\"prompt_eval_duration\", 0) / 1e9, 2),\n",
        "                        \"tokens\": generate_response_data.get(\"eval_count\", 0),\n",
        "                        \"tokens_per_sec\": 0\n",
        "                    }\n",
        "\n",
        "                    eval_duration = generate_response_data.get(\"eval_duration\", 0)\n",
        "                    if eval_duration > 0:\n",
        "                        test_metrics[\"tokens_per_sec\"] = round(\n",
        "                            test_metrics[\"tokens\"] / (eval_duration / 1e9), 2\n",
        "                        )\n",
        "\n",
        "                    metrics[\"tests\"].append(test_metrics)\n",
        "\n",
        "                    print(f\"  {C.DIM}‚Ä∫ {'Test':<7}{C.RESET} {test['name']}\")\n",
        "                    print(f\"  {C.DIM}‚Ä∫ {'Stats':<7}{C.RESET} {C.CYAN}{test_metrics['tokens_per_sec']:>6.2f} t/s{C.RESET} {C.DIM}| TTFT {test_metrics['first_token_time']:>5.2f}s | {test_metrics['tokens']:>4} tokens{C.RESET}\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"  {C.RED}‚Ä∫ {'Fail':<7}{C.RESET} HTTP {response.status_code}\")\n",
        "\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"  {C.RED}‚Ä∫ {'Error':<7}{C.RESET} {str(e)}\")\n",
        "\n",
        "        unload_model(model_name)\n",
        "        delete_model(model_name, env)\n",
        "\n",
        "        print(f\"  {C.DIM}‚Ä∫ {'Cleanup':<7}{C.RESET} Resources released\")\n",
        "        print(f\"  {C.GREEN}‚úÖ Pass{C.RESET}\")\n",
        "\n",
        "        successful_tests += 1\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        metrics[\"error\"] = \"Timeout exceeded\"\n",
        "        print(f\"  {C.RED}‚Ä∫ {'Error':<7}{C.RESET} Timeout exceeded ({timeout_seconds}s)\")\n",
        "        failed_tests += 1\n",
        "    except Exception as e:\n",
        "        metrics[\"error\"] = str(e)\n",
        "        print(f\"  {C.RED}‚Ä∫ {'Error':<7}{C.RESET} {str(e)}\")\n",
        "        failed_tests += 1\n",
        "\n",
        "    benchmark_result = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"model\": model_name,\n",
        "        \"environment\": sys_info,\n",
        "        \"metrics\": metrics\n",
        "    }\n",
        "\n",
        "    benchmark_results.append(benchmark_result)\n",
        "\n",
        "    if save_to_drive and RESULTS_FILE:\n",
        "        save_benchmark_result(benchmark_result, RESULTS_FILE)\n",
        "\n",
        "    print()\n",
        "\n",
        "print(f\"{C.BOLD}Benchmark Complete ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ{C.RESET}\")\n",
        "print(f\"{C.DIM}Success: {C.GREEN}{successful_tests}{C.RESET}{C.DIM} | Failed: {C.RED}{failed_tests}{C.RESET}{C.DIM} | Total: {len(benchmark_results)}{C.RESET}\")\n",
        "print()\n",
        "\n",
        "if save_to_drive and benchmark_results and ARCHIVE_DIR:\n",
        "    try:\n",
        "        session_end_time = datetime.utcnow()\n",
        "        session_archive = {\n",
        "            \"session_id\": session_id,\n",
        "            \"started_at\": session_start_time.isoformat() + \"Z\",\n",
        "            \"completed_at\": session_end_time.isoformat() + \"Z\",\n",
        "            \"duration_seconds\": round((session_end_time - session_start_time).total_seconds(), 2),\n",
        "            \"system_info\": sys_info,\n",
        "            \"models_tested\": selected_models,\n",
        "            \"successful\": successful_tests,\n",
        "            \"failed\": failed_tests,\n",
        "            \"results\": benchmark_results\n",
        "        }\n",
        "\n",
        "        archive_file = f\"{ARCHIVE_DIR}/{session_id}_session.json\"\n",
        "        with open(archive_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(session_archive, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"{C.DIM}Archive{C.RESET}\")\n",
        "        print(f\"  {C.DIM}‚Ä∫ {archive_file}{C.RESET}\")\n",
        "        print()\n",
        "\n",
        "    except (IOError, OSError) as e:\n",
        "        print(f\"{C.YELLOW}‚Ä∫ Warning{C.RESET} Archive save error: {str(e)}\")\n",
        "        print()\n",
        "\n",
        "if benchmark_results:\n",
        "    successful_results = [\n",
        "        r for r in benchmark_results\n",
        "        if not r[\"metrics\"].get(\"error\") and r[\"metrics\"][\"tests\"]\n",
        "    ]\n",
        "\n",
        "    if successful_results:\n",
        "        successful_results.sort(key=lambda x: x[\"metrics\"][\"tests\"][0][\"tokens_per_sec\"], reverse=True)\n",
        "\n",
        "        fastest = successful_results[0]\n",
        "        most_responsive = min(successful_results, key=lambda x: x[\"metrics\"][\"tests\"][0][\"first_token_time\"])\n",
        "        quickest_setup = min(successful_results, key=lambda x: x[\"metrics\"][\"pull_time\"])\n",
        "\n",
        "        leaders_md = [\n",
        "            \"Category Leaders ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\",\n",
        "            \"| Category | Model | Score |\",\n",
        "            \"|:--- |:--- |:--- |\",\n",
        "            f\"| ‚ö° Fastest Generation | {fastest['model']} | {fastest['metrics']['tests'][0]['tokens_per_sec']:.2f} t/s |\",\n",
        "            f\"| ‚è±Ô∏è Most Responsive | {most_responsive['model']} | {most_responsive['metrics']['tests'][0]['first_token_time']:.2f} s |\",\n",
        "            f\"| üì• Quickest Pull | {quickest_setup['model']} | {quickest_setup['metrics']['pull_time']:.2f} s |\"\n",
        "        ]\n",
        "        display(Markdown(\"\\n\".join(leaders_md)))\n",
        "        print()\n",
        "\n",
        "        detail_md = [\n",
        "            \"Detailed Metrics ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\",\n",
        "            \"| Model | Speed | TTFT | Total | Tok | Pull | Load | Size |\",\n",
        "            \"|:--- |---:|---:|---:|---:|---:|---:|---:|\",\n",
        "        ]\n",
        "\n",
        "        for result in successful_results:\n",
        "            m = result[\"metrics\"]\n",
        "            t = m[\"tests\"][0]\n",
        "            size_val = m.get(\"model_size_gb\", 0)\n",
        "\n",
        "            row = [\n",
        "                f\"`{result['model']}`\",\n",
        "                f\"{t['tokens_per_sec']:.2f} t/s\",\n",
        "                f\"{t['first_token_time']:.2f}s\",\n",
        "                f\"{t['total_time']:.2f}s\",\n",
        "                str(t['tokens']),\n",
        "                f\"{m['pull_time']:.1f}s\",\n",
        "                f\"{m['model_load_time']:.1f}s\",\n",
        "                f\"{size_val}GB\"\n",
        "            ]\n",
        "            detail_md.append(f\"| {' | '.join(row)} |\")\n",
        "\n",
        "        display(Markdown(\"\\n\".join(detail_md)))\n",
        "        print()\n",
        "\n",
        "        try:\n",
        "            print(f\"{C.BOLD}Graph Display ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ{C.RESET}\")\n",
        "\n",
        "            plot_data = successful_results[::-1]\n",
        "            num_models = len(plot_data)\n",
        "\n",
        "            p_models = [r['model'] for r in plot_data]\n",
        "            p_speeds = [r['metrics']['tests'][0]['tokens_per_sec'] for r in plot_data]\n",
        "            p_ttft = [r['metrics']['tests'][0]['first_token_time'] for r in plot_data]\n",
        "            p_total = [r['metrics']['tests'][0]['total_time'] for r in plot_data]\n",
        "            p_load = [r['metrics']['model_load_time'] for r in plot_data]\n",
        "            p_pull = [r['metrics']['pull_time'] for r in plot_data]\n",
        "            p_sizes = [r['metrics'].get('model_size_gb', 0) for r in plot_data]\n",
        "\n",
        "            plt.style.use('default')\n",
        "            fig, axes = plt.subplots(3, 2, figsize=(18, 12))\n",
        "            fig.suptitle('Ollama Benchmark Results', fontsize=18, fontweight='bold', y=0.96)\n",
        "\n",
        "            colors = {\n",
        "                'speed': '#00897B',\n",
        "                'ttft': '#FB8C00',\n",
        "                'total': '#1E88E5',\n",
        "                'load': '#8E24AA',\n",
        "                'pull': '#546E7A',\n",
        "                'size': '#6D4C41'\n",
        "            }\n",
        "\n",
        "            def plot_smart_barh(ax, data, title, xlabel, color, num_models):\n",
        "                base_height = min(0.6, 0.8 / max(num_models, 1))\n",
        "\n",
        "                bars = ax.barh(p_models, data, color=color, alpha=0.85, height=base_height)\n",
        "\n",
        "                ax.set_title(title, fontsize=12, fontweight='bold', pad=10)\n",
        "                ax.set_xlabel(xlabel, fontsize=10, color='#333333')\n",
        "                ax.grid(axis='x', linestyle=':', alpha=0.6)\n",
        "\n",
        "                ax.spines['top'].set_visible(False)\n",
        "                ax.spines['right'].set_visible(False)\n",
        "                ax.spines['left'].set_color('#cccccc')\n",
        "                ax.spines['bottom'].set_color('#cccccc')\n",
        "\n",
        "                max_val = max(data) if data and max(data) > 0 else 1\n",
        "                offset = max_val * 0.01\n",
        "\n",
        "                for bar in bars:\n",
        "                    width = bar.get_width()\n",
        "                    ax.text(width + offset, bar.get_y() + bar.get_height()/2,\n",
        "                            f' {width:.2f}',\n",
        "                            ha='left', va='center', fontsize=9, fontweight='bold', color='#444444')\n",
        "\n",
        "                ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "            plot_smart_barh(axes[0, 0], p_speeds, 'Generation Speed', 'Tokens / Sec', colors['speed'], num_models)\n",
        "            plot_smart_barh(axes[0, 1], p_ttft, 'Time To First Token', 'Seconds', colors['ttft'], num_models)\n",
        "            plot_smart_barh(axes[1, 0], p_total, 'Total Processing Time', 'Seconds', colors['total'], num_models)\n",
        "            plot_smart_barh(axes[1, 1], p_load, 'Model Load Time (VRAM)', 'Seconds', colors['load'], num_models)\n",
        "            plot_smart_barh(axes[2, 0], p_pull, 'Model Download Time', 'Seconds', colors['pull'], num_models)\n",
        "            plot_smart_barh(axes[2, 1], p_sizes, 'Model Size (Disk/VRAM)', 'GB', colors['size'], num_models)\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{C.YELLOW}Visualization Error: {e}{C.RESET}\")\n",
        "\n",
        "        print()\n",
        "        print(f\"{C.BOLD}Model Responses (Preview) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ{C.RESET}\")\n",
        "        print()\n",
        "\n",
        "        for result in successful_results:\n",
        "            model_name = result[\"model\"]\n",
        "            test = result[\"metrics\"][\"tests\"][0]\n",
        "            resp_text = test.get(\"response\", \"\").strip()\n",
        "\n",
        "            limit = config.MAX_RESPONSE_DISPLAY_CHARS\n",
        "            is_truncated_char = len(resp_text) > limit\n",
        "            if is_truncated_char:\n",
        "                resp_text = resp_text[:limit]\n",
        "\n",
        "            lines = resp_text.splitlines()\n",
        "            line_limit = 25\n",
        "            display_lines = lines[:line_limit]\n",
        "            is_truncated_line = len(lines) > line_limit\n",
        "\n",
        "            print(f\"  {C.DIM}‚Ä∫{C.RESET} {C.BOLD}{model_name}{C.RESET}\")\n",
        "            for line in display_lines:\n",
        "                print(f\"    {C.DIM}|{C.RESET} {line}\")\n",
        "\n",
        "            if is_truncated_char or is_truncated_line:\n",
        "                print(f\"    {C.DIM}‚ãÆ ... (preview truncated){C.RESET}\")\n",
        "            print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}