{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiroaki-com/ollama-llm-benchmark/blob/main/ollama_multi_model_benchmarker_ja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtoltT_-8pnA"
      },
      "source": [
        "Google Colabç’°å¢ƒã§Ollamaãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è‡ªå‹•æ¯”è¼ƒã™ã‚‹ã€å®Œå…¨ç„¡æ–™ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n",
        "\n",
        "ä¸»ãªç‰¹å¾´:\n",
        "- ğŸ”„ æŸ”è»Ÿãªãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå…¥åŠ› + ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ç°¡å˜é¸æŠï¼‰\n",
        "- ğŸ¯ Single Source of Truthè¨­è¨ˆï¼ˆãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã¯1ç®‡æ‰€ã®ã¿ç·¨é›†ã§ç®¡ç†ãŒç°¡å˜ï¼‰\n",
        "- ğŸ“Š åŒ…æ‹¬çš„ãªæ€§èƒ½æŒ‡æ¨™ï¼ˆç”Ÿæˆé€Ÿåº¦ã€TTFTã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ç­‰ã‚’æ¸¬å®šï¼‰\n",
        "- ğŸ’¾ è‡ªå‹•çµæœä¿å­˜ï¼ˆçµ±åˆJSONã€ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã€ã‚µã‚¤ã‚ºã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’`Google Drive`ã®`MyDrive`é…ä¸‹ã«ä¿å­˜ï¼‰\n",
        "- ğŸ“ˆ è¦–è¦šåŒ–ãƒ¬ãƒãƒ¼ãƒˆï¼ˆã‚°ãƒ©ãƒ• + Markdownãƒ†ãƒ¼ãƒ–ãƒ« + ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§çµæœã‚’å³æ™‚è¡¨ç¤ºï¼‰\n",
        "\n",
        "ä½¿ã„æ–¹:\n",
        "1. ğŸ“‹`Model Registry`ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿\n",
        "2. âœ…`ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹`ã§ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚’é¸æŠ\n",
        "3. ğŸ§ª`Ollama Multi-Model Benchmarker`ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦æ¸¬å®šé–‹å§‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uP0AeHIo8pnC"
      },
      "outputs": [],
      "source": [
        "#@title ğŸ“‹ Model Registry\n",
        "\n",
        "# @markdown ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
        "# @markdown - ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ã€‚ãƒ¢ãƒ‡ãƒ«åã¯ https://ollama.com/search ã§æ¤œç´¢ã—æ­£å¼åç§°ã‚’ã”ç¢ºèªãã ã•ã„ã€‚\n",
        "# @markdown\n",
        "# @markdown - ğŸ’¡ `T4 GPU`ç’°å¢ƒä¸‹ã®é¸å®šç›®å®‰ : `8B`ï¼ˆæ¨å¥¨ï¼‰ | `14B`ï¼ˆå®Ÿç”¨å¯ï¼‰ | `20B`ä»¥ä¸Šï¼ˆä½é€Ÿï¼‰\n",
        "\n",
        "model_list = \"qwen3:8b, qwen3:14b, qwen2.5-coder:7b, qwen2.5-coder:14b, ministral-3:8b, ministral-3:14b, devstral-small-2:24b, gpt-oss:20b, deepseek-r1:8b\" #@param {type:\"string\"}\n",
        "\n",
        "AVAILABLE_MODELS = [\n",
        "    model.strip()\n",
        "    for model in model_list.split(',')\n",
        "    if model.strip()\n",
        "]\n",
        "\n",
        "if not AVAILABLE_MODELS:\n",
        "    raise ValueError(\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆãŒç©ºã§ã™ã€‚å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def create_model_selector():\n",
        "    checkboxes = []\n",
        "\n",
        "    select_all = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='âœ… Select All Models',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='300px')\n",
        "    )\n",
        "\n",
        "    model_checks = {}\n",
        "    for model in AVAILABLE_MODELS:\n",
        "        cb = widgets.Checkbox(\n",
        "            value=True,\n",
        "            description=model,\n",
        "            indent=False,\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=widgets.Layout(width='300px')\n",
        "        )\n",
        "        model_checks[model] = cb\n",
        "        checkboxes.append(cb)\n",
        "\n",
        "    def on_select_all_change(change):\n",
        "        for cb in checkboxes:\n",
        "            cb.value = change['new']\n",
        "\n",
        "    select_all.observe(on_select_all_change, names='value')\n",
        "\n",
        "    header = widgets.HTML('<h3>ğŸ“¦ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«</h3><p style=\"margin: 5px 0 10px 0; font-size: 13px;\">å…¨é¸æŠã€ã¾ãŸã¯å€‹åˆ¥ã«ãƒã‚§ãƒƒã‚¯ã‚’å¤–ã—ã¦ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚’çµã‚Šè¾¼ã‚ã¾ã™ã€‚é¸æŠå¾Œã€æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</p>')\n",
        "    separator = widgets.HTML('<hr style=\"margin: 10px 0;\">')\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        header,\n",
        "        select_all,\n",
        "        separator,\n",
        "        widgets.VBox(checkboxes, layout=widgets.Layout(padding='0 0 0 20px'))\n",
        "    ])\n",
        "\n",
        "    display(ui)\n",
        "\n",
        "    return select_all, model_checks\n",
        "\n",
        "select_all_widget, model_checkboxes = create_model_selector()\n",
        "\n",
        "print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(AVAILABLE_MODELS)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚\")\n",
        "print(\"â¡ï¸ æ¬¡ã®ã‚»ãƒ«ï¼ˆBenchmarkerï¼‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QwBlqGb08pnD"
      },
      "outputs": [],
      "source": [
        "#@title ğŸ§ª Ollama Multi-Model Benchmarker\n",
        "\n",
        "# @markdown ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š\n",
        "# @markdown - `save_to_drive` : çµæœã‚’Google Driveã«ä¿å­˜ã™ã‚‹å ´åˆã¯ `True` ã«è¨­å®š\n",
        "# @markdown - `timeout_seconds` : 1ãƒ¢ãƒ‡ãƒ«ã‚ãŸã‚Šã®æœ€å¤§å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰\n",
        "# @markdown - `custom_test_prompt` : ç©ºæ¬„ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆPythonã‚³ãƒ¼ãƒ‰ç”Ÿæˆï¼‰ã‚’ä½¿ç”¨\n",
        "# @markdown\n",
        "\n",
        "save_to_drive = True #@param {type:\"boolean\"}\n",
        "timeout_seconds = 1000 #@param {type:\"integer\"}\n",
        "custom_test_prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import shutil\n",
        "import warnings\n",
        "import atexit\n",
        "import psutil\n",
        "import platform\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    OLLAMA_BINARY: str = \"/usr/local/bin/ollama\"\n",
        "    OLLAMA_HOST: str = \"0.0.0.0:11434\"\n",
        "    OLLAMA_API_BASE: str = \"http://0.0.0.0:11434\"\n",
        "\n",
        "    PULL_MAX_RETRIES: int = 3\n",
        "    PULL_BACKOFF_BASE: int = 5\n",
        "\n",
        "    SERVER_STARTUP_MAX_ATTEMPTS: int = 30\n",
        "    SERVER_STARTUP_POLL_INTERVAL: int = 1\n",
        "    SERVER_HEALTH_CHECK_TIMEOUT: int = 2\n",
        "\n",
        "    WARMUP_TIMEOUT: int = 300\n",
        "    WARMUP_NUM_PREDICT: int = 1\n",
        "    MODEL_UNLOAD_WAIT: int = 2\n",
        "    MODEL_UNLOAD_TIMEOUT: int = 10\n",
        "\n",
        "    DEFAULT_PROMPT: str = \"Write a recursive Python function with type hints and a docstring to compute the factorial of a number, test it with n = 5, and show only the code and the expected result.\"\n",
        "    MAX_PROMPT_CHARS: int = 500\n",
        "    MAX_RESPONSE_DISPLAY_CHARS: int = 1500\n",
        "\n",
        "    DISK_SAFETY_MARGIN_GB: int = 2\n",
        "    UNKNOWN_MODEL_MIN_FREE_GB: int = 20\n",
        "\n",
        "    NUM_CTX: int = 4096\n",
        "    TEMPERATURE: float = 0.0\n",
        "\n",
        "    CACHE_FILENAME: str = \"model_size_cache.json\"\n",
        "\n",
        "class C:\n",
        "    RESET = '\\033[0m'\n",
        "    GREEN = '\\033[32m'\n",
        "    RED = '\\033[31m'\n",
        "    YELLOW = '\\033[33m'\n",
        "    BLUE = '\\033[34m'\n",
        "    CYAN = '\\033[36m'\n",
        "    MAGENTA = '\\033[35m'\n",
        "    WHITE = '\\033[37m'\n",
        "    DIM = '\\033[2m'\n",
        "    BOLD = '\\033[1m'\n",
        "\n",
        "config = Config()\n",
        "session_id = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "ollama_process: Optional[subprocess.Popen] = None\n",
        "\n",
        "try:\n",
        "    if select_all_widget.value:\n",
        "        selected_models = AVAILABLE_MODELS.copy()\n",
        "    else:\n",
        "        selected_models = [\n",
        "            model for model, checkbox in model_checkboxes.items()\n",
        "            if checkbox.value\n",
        "        ]\n",
        "except NameError:\n",
        "    print(f\"{C.RED}âŒ ã‚¨ãƒ©ãƒ¼: Model RegistryãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“{C.RESET}\")\n",
        "    print(f\"{C.YELLOW}å…ˆã« 'Model Registry' ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚{C.RESET}\")\n",
        "    raise SystemExit(\"Model Registry cell must be executed before benchmark\")\n",
        "\n",
        "if not selected_models:\n",
        "    print(f\"{C.RED}âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“{C.RESET}\")\n",
        "    print(f\"{C.YELLOW}Model Registryã‚»ãƒ«ã§å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚{C.RESET}\")\n",
        "    raise SystemExit(\"At least one model must be selected\")\n",
        "\n",
        "def cleanup_ollama_server() -> None:\n",
        "    global ollama_process\n",
        "    if ollama_process:\n",
        "        try:\n",
        "            ollama_process.terminate()\n",
        "            ollama_process.wait(timeout=5)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            try:\n",
        "                ollama_process.kill()\n",
        "            except ProcessLookupError:\n",
        "                pass\n",
        "        except ProcessLookupError:\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(f\"{C.YELLOW}Warning: Failed to cleanup Ollama process: {e}{C.RESET}\", file=sys.stderr)\n",
        "\n",
        "def trim_to_boundary(text: str, limit: int) -> str:\n",
        "    if len(text) <= limit:\n",
        "        return text\n",
        "    candidate = text[:limit]\n",
        "    for sep in [\"\\n\", \"ã€‚\", \".\", \"ã€\", \",\", \" \"]:\n",
        "        idx = candidate.rfind(sep)\n",
        "        if idx > limit // 2:\n",
        "            return candidate[:idx + len(sep)].rstrip()\n",
        "    return candidate.rstrip()\n",
        "\n",
        "if custom_test_prompt.strip():\n",
        "    resolved_prompt = trim_to_boundary(custom_test_prompt.strip(), config.MAX_PROMPT_CHARS)\n",
        "else:\n",
        "    resolved_prompt = config.DEFAULT_PROMPT\n",
        "\n",
        "TEST_PROMPTS: List[Dict[str, str]] = [\n",
        "    {\n",
        "        \"name\": \"Python Factorial\" if not custom_test_prompt.strip() else \"Custom Prompt\",\n",
        "        \"prompt\": resolved_prompt,\n",
        "        \"expected\": \"Recursive factorial function\" if not custom_test_prompt.strip() else \"Custom\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Ollama Multi-Model Benchmarker\")\n",
        "print(f\"Models: {len(selected_models)} | Timeout: {timeout_seconds}s\")\n",
        "if custom_test_prompt.strip() and len(custom_test_prompt.strip()) != len(resolved_prompt):\n",
        "    print(f\"{C.YELLOW}  â€º ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¢ƒç•Œã§åˆ‡ã‚Šè©°ã‚ã¾ã—ãŸ ({len(resolved_prompt)} chars){C.RESET}\")\n",
        "print()\n",
        "print(\"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«:\")\n",
        "for idx, model in enumerate(selected_models, 1):\n",
        "    print(f\"  {idx}. {model}\")\n",
        "print()\n",
        "print(\"ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©:\")\n",
        "print(f\"  {'t/s':<10} : Tokens per Second ... ç”Ÿæˆé€Ÿåº¦\")\n",
        "print(f\"  {'TTFT':<10} : Time To First Token . å¿œç­”é…å»¶\")\n",
        "print(f\"  {'Total':<10} : End-to-End Time ..... å®Œå…¨å‡¦ç†æ™‚é–“\")\n",
        "print(f\"  {'Size':<10} : Model Size .......... Disk/VRAMä½¿ç”¨é‡\")\n",
        "print()\n",
        "\n",
        "if save_to_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    BASE_DIR = '/content/drive/MyDrive/OllamaBenchmarks'\n",
        "    RESULTS_FILE = f'{BASE_DIR}/benchmark_results.json'\n",
        "    ARCHIVE_DIR = f'{BASE_DIR}/session_logs'\n",
        "    CACHE_FILE = f'{BASE_DIR}/{config.CACHE_FILENAME}'\n",
        "\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n",
        "else:\n",
        "    BASE_DIR = None\n",
        "    RESULTS_FILE = None\n",
        "    ARCHIVE_DIR = None\n",
        "    CACHE_FILE = None\n",
        "\n",
        "def load_size_cache() -> Dict[str, float]:\n",
        "    if save_to_drive and CACHE_FILE and os.path.exists(CACHE_FILE):\n",
        "        try:\n",
        "            with open(CACHE_FILE, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"{C.YELLOW}Warning: Failed to load size cache: {e}{C.RESET}\")\n",
        "    return {}\n",
        "\n",
        "def update_size_cache(model_name: str, size_gb: float) -> None:\n",
        "    if not save_to_drive or not CACHE_FILE:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        cache = load_size_cache()\n",
        "        cache[model_name] = size_gb\n",
        "        with open(CACHE_FILE, 'w') as f:\n",
        "            json.dump(cache, f, indent=2)\n",
        "    except Exception as e:\n",
        "        print(f\"{C.YELLOW}Warning: Failed to update size cache: {e}{C.RESET}\")\n",
        "\n",
        "def get_disk_usage() -> Dict[str, float]:\n",
        "    total, used, free = shutil.disk_usage(\"/\")\n",
        "    return {\n",
        "        \"total_gb\": round(total / (1024**3), 2),\n",
        "        \"used_gb\": round(used / (1024**3), 2),\n",
        "        \"free_gb\": round(free / (1024**3), 2)\n",
        "    }\n",
        "\n",
        "def get_cpu_info() -> str:\n",
        "    try:\n",
        "        if platform.system() == \"Linux\":\n",
        "            with open(\"/proc/cpuinfo\", \"r\") as f:\n",
        "                for line in f:\n",
        "                    if \"model name\" in line:\n",
        "                        return line.split(\":\")[1].strip()\n",
        "        return platform.processor() or \"Unknown CPU\"\n",
        "    except:\n",
        "        return \"Unknown CPU\"\n",
        "\n",
        "def get_system_info() -> Dict[str, Any]:\n",
        "    try:\n",
        "        gpu_info = !nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits\n",
        "        if gpu_info:\n",
        "            parts = gpu_info[0].split(',')\n",
        "            gpu_name = parts[0].strip()\n",
        "            vram_gb = int(parts[1].strip())\n",
        "        else:\n",
        "            gpu_name = \"Unknown/None\"\n",
        "            vram_gb = 0\n",
        "    except Exception as e:\n",
        "        gpu_name = \"Unknown\"\n",
        "        vram_gb = 0\n",
        "\n",
        "    cpu_name = get_cpu_info()\n",
        "    cpu_cores = os.cpu_count()\n",
        "    try:\n",
        "        ram_obj = psutil.virtual_memory()\n",
        "        ram_total_gb = round(ram_obj.total / (1024**3), 2)\n",
        "        ram_available_gb = round(ram_obj.available / (1024**3), 2)\n",
        "    except Exception:\n",
        "        ram_total_gb = 0\n",
        "        ram_available_gb = 0\n",
        "\n",
        "    disk_info = get_disk_usage()\n",
        "\n",
        "    return {\n",
        "        \"gpu\": gpu_name,\n",
        "        \"vram_gb\": vram_gb,\n",
        "        \"cpu\": cpu_name,\n",
        "        \"cpu_cores\": cpu_cores,\n",
        "        \"ram_total_gb\": ram_total_gb,\n",
        "        \"ram_available_gb\": ram_available_gb,\n",
        "        \"disk_total_gb\": disk_info[\"total_gb\"],\n",
        "        \"disk_free_gb\": disk_info[\"free_gb\"],\n",
        "        \"platform\": platform.platform()\n",
        "    }\n",
        "\n",
        "def get_model_details(model_name: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/show\",\n",
        "            json={\"name\": model_name},\n",
        "            timeout=5\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            generate_response_data = response.json()\n",
        "            details = generate_response_data.get(\"details\", {})\n",
        "            return {\n",
        "                \"quantization\": details.get(\"quantization_level\", \"Unknown\"),\n",
        "                \"family\": details.get(\"family\", \"Unknown\"),\n",
        "                \"parameter_size\": details.get(\"parameter_size\", \"Unknown\")\n",
        "            }\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {\"quantization\": \"Unknown\", \"family\": \"Unknown\", \"parameter_size\": \"Unknown\"}\n",
        "\n",
        "def get_installed_model_size(model_name: str) -> Optional[float]:\n",
        "    try:\n",
        "        response = requests.get(f\"{config.OLLAMA_API_BASE}/api/tags\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            models = response.json().get(\"models\", [])\n",
        "            for m in models:\n",
        "                if m[\"name\"] == model_name or m[\"name\"] == f\"{model_name}:latest\":\n",
        "                    size_bytes = m.get(\"size\", 0)\n",
        "                    return round(size_bytes / (1024**3), 2)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq zstd\n",
        "!pip install -q psutil matplotlib ipywidgets\n",
        "\n",
        "print()\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print()\n",
        "\n",
        "os.environ['OLLAMA_HOST'] = config.OLLAMA_HOST\n",
        "os.environ['OLLAMA_KEEP_ALIVE'] = '5m'\n",
        "os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'\n",
        "os.environ['OLLAMA_FLASH_ATTENTION'] = '1'\n",
        "\n",
        "startup_start = time.time()\n",
        "\n",
        "ollama_process = subprocess.Popen(\n",
        "    [config.OLLAMA_BINARY, \"serve\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "for attempt in range(config.SERVER_STARTUP_MAX_ATTEMPTS):\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/tags\",\n",
        "            timeout=config.SERVER_HEALTH_CHECK_TIMEOUT\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            startup_time = round(time.time() - startup_start, 2)\n",
        "            print(f\"{C.GREEN}âœ… Ollama server ready in {startup_time}s{C.RESET}\")\n",
        "            atexit.register(cleanup_ollama_server)\n",
        "            break\n",
        "    except requests.RequestException:\n",
        "        pass\n",
        "    time.sleep(config.SERVER_STARTUP_POLL_INTERVAL)\n",
        "else:\n",
        "    raise RuntimeError(\"âŒ ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
        "\n",
        "sys_info = get_system_info()\n",
        "print()\n",
        "print(f\"{C.BOLD}ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{C.RESET}\")\n",
        "print(f\"  OS   : {sys_info['platform']}\")\n",
        "print(f\"  CPU  : {sys_info['cpu']} ({sys_info['cpu_cores']} cores)\")\n",
        "print(f\"  RAM  : {sys_info['ram_total_gb']} GB\")\n",
        "print(f\"  GPU  : {sys_info['gpu']} ({sys_info['vram_gb']} GB VRAM)\")\n",
        "print()\n",
        "\n",
        "def pull_model_with_retry(model_name: str, env: Dict[str, str], timeout: int) -> subprocess.CompletedProcess:\n",
        "    for attempt in range(1, config.PULL_MAX_RETRIES + 1):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [config.OLLAMA_BINARY, \"pull\", model_name],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=timeout,\n",
        "                env=env\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                return result\n",
        "            if attempt < config.PULL_MAX_RETRIES:\n",
        "                wait = config.PULL_BACKOFF_BASE * (2 ** (attempt - 1))\n",
        "                print(f\"  {C.YELLOW}â€º {'Retry':<7}{C.RESET} Pullå¤±æ•— (è©¦è¡Œ {attempt}/{config.PULL_MAX_RETRIES}) â€” {wait}ç§’å¾Œã«å†è©¦è¡Œ â€¦\")\n",
        "                time.sleep(wait)\n",
        "        except subprocess.TimeoutExpired as e:\n",
        "            if attempt >= config.PULL_MAX_RETRIES:\n",
        "                raise\n",
        "            wait = config.PULL_BACKOFF_BASE * (2 ** (attempt - 1))\n",
        "            print(f\"  {C.YELLOW}â€º {'Retry':<7}{C.RESET} Pullã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (è©¦è¡Œ {attempt}/{config.PULL_MAX_RETRIES}) â€” {wait}ç§’å¾Œã«å†è©¦è¡Œ â€¦\")\n",
        "            time.sleep(wait)\n",
        "    raise RuntimeError(f\"{config.PULL_MAX_RETRIES}å›ã®è©¦è¡Œå¾Œã«PullãŒå¤±æ•—ã—ã¾ã—ãŸ: {result.stderr}\")\n",
        "\n",
        "def warmup_model(model_name: str) -> float:\n",
        "    try:\n",
        "        warmup_res = requests.post(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/generate\",\n",
        "            json={\n",
        "                \"model\": model_name,\n",
        "                \"prompt\": \"warmup\",\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"num_ctx\": config.NUM_CTX,\n",
        "                    \"num_predict\": config.WARMUP_NUM_PREDICT\n",
        "                }\n",
        "            },\n",
        "            timeout=config.WARMUP_TIMEOUT\n",
        "        )\n",
        "        if warmup_res.status_code == 200:\n",
        "            warmup_data = warmup_res.json()\n",
        "            return round(warmup_data.get(\"total_duration\", 0) / 1e9, 2)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"  {C.YELLOW}â€º {'Warning':<7}{C.RESET} Warmupå¤±æ•—: {e}\", file=sys.stderr)\n",
        "    return 0.0\n",
        "\n",
        "def unload_model(model_name: str) -> None:\n",
        "    try:\n",
        "        requests.post(\n",
        "            f\"{config.OLLAMA_API_BASE}/api/generate\",\n",
        "            json={\"model\": model_name, \"keep_alive\": 0},\n",
        "            timeout=config.MODEL_UNLOAD_TIMEOUT\n",
        "        )\n",
        "        time.sleep(config.MODEL_UNLOAD_WAIT)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"  {C.YELLOW}â€º {'Warning':<7}{C.RESET} Unloadå¤±æ•—: {e}\", file=sys.stderr)\n",
        "\n",
        "def delete_model(model_name: str, env: Dict[str, str]) -> None:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [config.OLLAMA_BINARY, \"rm\", model_name],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            env=env,\n",
        "            timeout=30\n",
        "        )\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        print(f\"  {C.YELLOW}â€º {'Warning':<7}{C.RESET} Deleteã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}\", file=sys.stderr)\n",
        "\n",
        "def save_benchmark_result(benchmark_result: Dict[str, Any], results_file: str) -> None:\n",
        "    try:\n",
        "        if os.path.exists(results_file):\n",
        "            with open(results_file, 'r', encoding='utf-8') as f:\n",
        "                all_data = json.load(f)\n",
        "        else:\n",
        "            all_data = {\n",
        "                \"schema_version\": \"1.1\",\n",
        "                \"last_updated\": None,\n",
        "                \"benchmarks\": []\n",
        "            }\n",
        "\n",
        "        all_data[\"benchmarks\"].append(benchmark_result)\n",
        "        all_data[\"last_updated\"] = datetime.utcnow().isoformat() + \"Z\"\n",
        "\n",
        "        temp_file = results_file + \".tmp\"\n",
        "        with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "        os.replace(temp_file, results_file)\n",
        "\n",
        "    except (IOError, json.JSONDecodeError) as e:\n",
        "        print(f\"  {C.YELLOW}â€º {'Warning':<7}{C.RESET} ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
        "\n",
        "session_start_time = datetime.utcnow()\n",
        "benchmark_results: List[Dict[str, Any]] = []\n",
        "successful_tests = 0\n",
        "failed_tests = 0\n",
        "\n",
        "model_size_cache = load_size_cache()\n",
        "\n",
        "selected_models.sort(key=lambda m: model_size_cache.get(m, 0))\n",
        "\n",
        "for model_idx, model_name in enumerate(selected_models, 1):\n",
        "    print(f\"{C.BLUE}â–¶{C.RESET} {C.BOLD}[{model_idx}/{len(selected_models)}] {model_name}{C.RESET}\")\n",
        "\n",
        "    disk_before = get_disk_usage()\n",
        "\n",
        "    cached_size = model_size_cache.get(model_name)\n",
        "    if cached_size is not None:\n",
        "        required_space = cached_size + config.DISK_SAFETY_MARGIN_GB\n",
        "        size_str = f\"{cached_size}GB (cached)\"\n",
        "    else:\n",
        "        required_space = config.UNKNOWN_MODEL_MIN_FREE_GB\n",
        "        size_str = \"Unknown (Checking >20GB)\"\n",
        "\n",
        "    if disk_before['free_gb'] < required_space:\n",
        "        print(f\"  {C.RED}â€º {'Skip':<7}{C.RESET} ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ (å¿…è¦ {required_space:.1f}GB, ç©ºã {disk_before['free_gb']}GB)\")\n",
        "\n",
        "        failed_metrics = {\n",
        "            \"model\": model_name,\n",
        "            \"error\": \"Insufficient disk space\",\n",
        "            \"required_gb\": required_space,\n",
        "            \"free_gb\": disk_before['free_gb']\n",
        "        }\n",
        "        benchmark_results.append({\n",
        "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"model\": model_name,\n",
        "            \"environment\": sys_info,\n",
        "            \"metrics\": failed_metrics\n",
        "        })\n",
        "        failed_tests += 1\n",
        "        print()\n",
        "        continue\n",
        "\n",
        "    metrics: Dict[str, Any] = {\n",
        "        \"model\": model_name,\n",
        "        \"pull_time\": 0,\n",
        "        \"model_load_time\": 0,\n",
        "        \"model_size_gb\": cached_size if cached_size else 0,\n",
        "        \"meta\": {},\n",
        "        \"tests\": [],\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"  {C.DIM}â€º {'Setup':<7}{C.RESET} Free: {disk_before['free_gb']}GB | Est. Size: {size_str}\")\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        env['OLLAMA_HOST'] = config.OLLAMA_HOST\n",
        "        env['HOME'] = '/root'\n",
        "\n",
        "        pull_start = time.time()\n",
        "        pull_model_with_retry(model_name, env, timeout_seconds)\n",
        "        metrics[\"pull_time\"] = round(time.time() - pull_start, 2)\n",
        "\n",
        "        model_details = get_model_details(model_name)\n",
        "        metrics[\"meta\"] = model_details\n",
        "        quant_disp = model_details['quantization']\n",
        "\n",
        "        real_size = get_installed_model_size(model_name)\n",
        "        if real_size:\n",
        "            metrics[\"model_size_gb\"] = real_size\n",
        "            if cached_size != real_size:\n",
        "                update_size_cache(model_name, real_size)\n",
        "                model_size_cache[model_name] = real_size\n",
        "            size_display = f\"{real_size}GB\"\n",
        "        else:\n",
        "            size_display = \"Unknown\"\n",
        "\n",
        "        print(f\"  {C.DIM}â€º {'Pull':<7}{C.RESET} {C.GREEN}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† {metrics['pull_time']}s{C.RESET} {C.DIM}({size_display}, {quant_disp}){C.RESET}\")\n",
        "\n",
        "        metrics[\"model_load_time\"] = warmup_model(model_name)\n",
        "        print(f\"  {C.DIM}â€º {'Load':<7}{C.RESET} {C.GREEN}VRAMãƒ­ãƒ¼ãƒ‰å®Œäº† {metrics['model_load_time']}s{C.RESET}\")\n",
        "\n",
        "        for test_idx, test in enumerate(TEST_PROMPTS, 1):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    f\"{config.OLLAMA_API_BASE}/api/generate\",\n",
        "                    json={\n",
        "                        \"model\": model_name,\n",
        "                        \"prompt\": test[\"prompt\"],\n",
        "                        \"stream\": False,\n",
        "                        \"options\": {\n",
        "                            \"num_ctx\": config.NUM_CTX,\n",
        "                            \"temperature\": config.TEMPERATURE\n",
        "                        }\n",
        "                    },\n",
        "                    timeout=timeout_seconds\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    generate_response_data = response.json()\n",
        "                    response_text = generate_response_data.get(\"response\", \"\")\n",
        "\n",
        "                    test_metrics = {\n",
        "                        \"name\": test[\"name\"],\n",
        "                        \"prompt\": test[\"prompt\"],\n",
        "                        \"response\": response_text,\n",
        "                        \"total_time\": round(generate_response_data.get(\"total_duration\", 0) / 1e9, 2),\n",
        "                        \"first_token_time\": round(generate_response_data.get(\"prompt_eval_duration\", 0) / 1e9, 2),\n",
        "                        \"tokens\": generate_response_data.get(\"eval_count\", 0),\n",
        "                        \"tokens_per_sec\": 0\n",
        "                    }\n",
        "\n",
        "                    eval_duration = generate_response_data.get(\"eval_duration\", 0)\n",
        "                    if eval_duration > 0:\n",
        "                        test_metrics[\"tokens_per_sec\"] = round(\n",
        "                            test_metrics[\"tokens\"] / (eval_duration / 1e9), 2\n",
        "                        )\n",
        "\n",
        "                    metrics[\"tests\"].append(test_metrics)\n",
        "\n",
        "                    print(f\"  {C.DIM}â€º {'Test':<7}{C.RESET} {test['name']}\")\n",
        "                    print(f\"  {C.DIM}â€º {'Stats':<7}{C.RESET} {C.CYAN}{test_metrics['tokens_per_sec']:>6.2f} t/s{C.RESET} {C.DIM}| TTFT {test_metrics['first_token_time']:>5.2f}s | {test_metrics['tokens']:>4} tokens{C.RESET}\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"  {C.RED}â€º {'Fail':<7}{C.RESET} HTTP {response.status_code}\")\n",
        "\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"  {C.RED}â€º {'Error':<7}{C.RESET} {str(e)}\")\n",
        "\n",
        "        unload_model(model_name)\n",
        "        delete_model(model_name, env)\n",
        "\n",
        "        print(f\"  {C.DIM}â€º {'Cleanup':<7}{C.RESET} ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾å®Œäº†\")\n",
        "        print(f\"  {C.GREEN}âœ… Pass{C.RESET}\")\n",
        "\n",
        "        successful_tests += 1\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        metrics[\"error\"] = \"Timeout exceeded\"\n",
        "        print(f\"  {C.RED}â€º {'Error':<7}{C.RESET} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¶…é ({timeout_seconds}s)\")\n",
        "        failed_tests += 1\n",
        "    except Exception as e:\n",
        "        metrics[\"error\"] = str(e)\n",
        "        print(f\"  {C.RED}â€º {'Error':<7}{C.RESET} {str(e)}\")\n",
        "        failed_tests += 1\n",
        "\n",
        "    benchmark_result = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"model\": model_name,\n",
        "        \"environment\": sys_info,\n",
        "        \"metrics\": metrics\n",
        "    }\n",
        "\n",
        "    benchmark_results.append(benchmark_result)\n",
        "\n",
        "    if save_to_drive and RESULTS_FILE:\n",
        "        save_benchmark_result(benchmark_result, RESULTS_FILE)\n",
        "\n",
        "    print()\n",
        "\n",
        "print(f\"{C.BOLD}ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{C.RESET}\")\n",
        "print(f\"{C.DIM}æˆåŠŸ: {C.GREEN}{successful_tests}{C.RESET}{C.DIM} | å¤±æ•—: {C.RED}{failed_tests}{C.RESET}{C.DIM} | åˆè¨ˆ: {len(benchmark_results)}{C.RESET}\")\n",
        "print()\n",
        "\n",
        "if save_to_drive and benchmark_results and ARCHIVE_DIR:\n",
        "    try:\n",
        "        session_end_time = datetime.utcnow()\n",
        "        session_archive = {\n",
        "            \"session_id\": session_id,\n",
        "            \"started_at\": session_start_time.isoformat() + \"Z\",\n",
        "            \"completed_at\": session_end_time.isoformat() + \"Z\",\n",
        "            \"duration_seconds\": round((session_end_time - session_start_time).total_seconds(), 2),\n",
        "            \"system_info\": sys_info,\n",
        "            \"models_tested\": selected_models,\n",
        "            \"successful\": successful_tests,\n",
        "            \"failed\": failed_tests,\n",
        "            \"results\": benchmark_results\n",
        "        }\n",
        "\n",
        "        archive_file = f\"{ARCHIVE_DIR}/{session_id}_session.json\"\n",
        "        with open(archive_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(session_archive, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"{C.DIM}Archive{C.RESET}\")\n",
        "        print(f\"  {C.DIM}â€º {archive_file}{C.RESET}\")\n",
        "        print()\n",
        "\n",
        "    except (IOError, OSError) as e:\n",
        "        print(f\"{C.YELLOW}â€º Warning{C.RESET} ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
        "        print()\n",
        "\n",
        "if benchmark_results:\n",
        "    successful_results = [\n",
        "        r for r in benchmark_results\n",
        "        if not r[\"metrics\"].get(\"error\") and r[\"metrics\"][\"tests\"]\n",
        "    ]\n",
        "\n",
        "    if successful_results:\n",
        "        successful_results.sort(key=lambda x: x[\"metrics\"][\"tests\"][0][\"tokens_per_sec\"], reverse=True)\n",
        "\n",
        "        fastest = successful_results[0]\n",
        "        most_responsive = min(successful_results, key=lambda x: x[\"metrics\"][\"tests\"][0][\"first_token_time\"])\n",
        "        quickest_setup = min(successful_results, key=lambda x: x[\"metrics\"][\"pull_time\"])\n",
        "\n",
        "        leaders_md = [\n",
        "            \"ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒˆãƒƒãƒ— â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\",\n",
        "            \"| Category | Model | Score |\",\n",
        "            \"|:--- |:--- |:--- |\",\n",
        "            f\"| âš¡ Fastest Generation | {fastest['model']} | {fastest['metrics']['tests'][0]['tokens_per_sec']:.2f} t/s |\",\n",
        "            f\"| â±ï¸ Most Responsive | {most_responsive['model']} | {most_responsive['metrics']['tests'][0]['first_token_time']:.2f} s |\",\n",
        "            f\"| ğŸ“¥ Quickest Pull | {quickest_setup['model']} | {quickest_setup['metrics']['pull_time']:.2f} s |\"\n",
        "        ]\n",
        "        display(Markdown(\"\\n\".join(leaders_md)))\n",
        "        print()\n",
        "\n",
        "        detail_md = [\n",
        "            \"è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\",\n",
        "            \"| Model | Speed | TTFT | Total | Tok | Pull | Load | Size |\",\n",
        "            \"|:--- |---:|---:|---:|---:|---:|---:|---:|\",\n",
        "        ]\n",
        "\n",
        "        for result in successful_results:\n",
        "            m = result[\"metrics\"]\n",
        "            t = m[\"tests\"][0]\n",
        "            size_val = m.get(\"model_size_gb\", 0)\n",
        "\n",
        "            row = [\n",
        "                f\"`{result['model']}`\",\n",
        "                f\"{t['tokens_per_sec']:.2f} t/s\",\n",
        "                f\"{t['first_token_time']:.2f}s\",\n",
        "                f\"{t['total_time']:.2f}s\",\n",
        "                str(t['tokens']),\n",
        "                f\"{m['pull_time']:.1f}s\",\n",
        "                f\"{m['model_load_time']:.1f}s\",\n",
        "                f\"{size_val}GB\"\n",
        "            ]\n",
        "            detail_md.append(f\"| {' | '.join(row)} |\")\n",
        "\n",
        "        display(Markdown(\"\\n\".join(detail_md)))\n",
        "        print()\n",
        "\n",
        "        try:\n",
        "            print(f\"{C.BOLD}ã‚°ãƒ©ãƒ•è¡¨ç¤º â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{C.RESET}\")\n",
        "\n",
        "            plot_data = successful_results[::-1]\n",
        "            num_models = len(plot_data)\n",
        "\n",
        "            p_models = [r['model'] for r in plot_data]\n",
        "            p_speeds = [r['metrics']['tests'][0]['tokens_per_sec'] for r in plot_data]\n",
        "            p_ttft = [r['metrics']['tests'][0]['first_token_time'] for r in plot_data]\n",
        "            p_total = [r['metrics']['tests'][0]['total_time'] for r in plot_data]\n",
        "            p_load = [r['metrics']['model_load_time'] for r in plot_data]\n",
        "            p_pull = [r['metrics']['pull_time'] for r in plot_data]\n",
        "            p_sizes = [r['metrics'].get('model_size_gb', 0) for r in plot_data]\n",
        "\n",
        "            plt.style.use('default')\n",
        "            fig, axes = plt.subplots(3, 2, figsize=(18, 12))\n",
        "            fig.suptitle('Ollama Benchmark Results', fontsize=18, fontweight='bold', y=0.96)\n",
        "\n",
        "            colors = {\n",
        "                'speed': '#00897B',\n",
        "                'ttft': '#FB8C00',\n",
        "                'total': '#1E88E5',\n",
        "                'load': '#8E24AA',\n",
        "                'pull': '#546E7A',\n",
        "                'size': '#6D4C41'\n",
        "            }\n",
        "\n",
        "            def plot_smart_barh(ax, data, title, xlabel, color, num_models):\n",
        "                base_height = min(0.6, 0.8 / max(num_models, 1))\n",
        "\n",
        "                bars = ax.barh(p_models, data, color=color, alpha=0.85, height=base_height)\n",
        "\n",
        "                ax.set_title(title, fontsize=12, fontweight='bold', pad=10)\n",
        "                ax.set_xlabel(xlabel, fontsize=10, color='#333333')\n",
        "                ax.grid(axis='x', linestyle=':', alpha=0.6)\n",
        "\n",
        "                ax.spines['top'].set_visible(False)\n",
        "                ax.spines['right'].set_visible(False)\n",
        "                ax.spines['left'].set_color('#cccccc')\n",
        "                ax.spines['bottom'].set_color('#cccccc')\n",
        "\n",
        "                max_val = max(data) if data and max(data) > 0 else 1\n",
        "                offset = max_val * 0.01\n",
        "\n",
        "                for bar in bars:\n",
        "                    width = bar.get_width()\n",
        "                    ax.text(width + offset, bar.get_y() + bar.get_height()/2,\n",
        "                            f' {width:.2f}',\n",
        "                            ha='left', va='center', fontsize=9, fontweight='bold', color='#444444')\n",
        "\n",
        "                ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "            plot_smart_barh(axes[0, 0], p_speeds, 'Generation Speed', 'Tokens / Sec', colors['speed'], num_models)\n",
        "            plot_smart_barh(axes[0, 1], p_ttft, 'Time To First Token', 'Seconds', colors['ttft'], num_models)\n",
        "            plot_smart_barh(axes[1, 0], p_total, 'Total Processing Time', 'Seconds', colors['total'], num_models)\n",
        "            plot_smart_barh(axes[1, 1], p_load, 'Model Load Time (VRAM)', 'Seconds', colors['load'], num_models)\n",
        "            plot_smart_barh(axes[2, 0], p_pull, 'Model Download Time', 'Seconds', colors['pull'], num_models)\n",
        "            plot_smart_barh(axes[2, 1], p_sizes, 'Model Size (Disk/VRAM)', 'GB', colors['size'], num_models)\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{C.YELLOW}Visualization Error: {e}{C.RESET}\")\n",
        "\n",
        "        print()\n",
        "        print(f\"{C.BOLD}ãƒ¢ãƒ‡ãƒ«ã®å¿œç­” (Preview) â”â”â”â”â”â”â”â”â”â”â”â”â”â”{C.RESET}\")\n",
        "        print()\n",
        "\n",
        "        for result in successful_results:\n",
        "            model_name = result[\"model\"]\n",
        "            test = result[\"metrics\"][\"tests\"][0]\n",
        "            resp_text = test.get(\"response\", \"\").strip()\n",
        "\n",
        "            limit = config.MAX_RESPONSE_DISPLAY_CHARS\n",
        "            is_truncated_char = len(resp_text) > limit\n",
        "            if is_truncated_char:\n",
        "                resp_text = resp_text[:limit]\n",
        "\n",
        "            lines = resp_text.splitlines()\n",
        "            line_limit = 25\n",
        "            display_lines = lines[:line_limit]\n",
        "            is_truncated_line = len(lines) > line_limit\n",
        "\n",
        "            print(f\"  {C.DIM}â€º{C.RESET} {C.BOLD}{model_name}{C.RESET}\")\n",
        "            for line in display_lines:\n",
        "                print(f\"    {C.DIM}|{C.RESET} {line}\")\n",
        "\n",
        "            if is_truncated_char or is_truncated_line:\n",
        "                print(f\"    {C.DIM}â‹® ... (preview truncated){C.RESET}\")\n",
        "            print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}